{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1f2172-9c8d-47d3-b332-52bcefb2763f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Short tour of Julia for GPU programming and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10726e68-972a-47ac-8da9-24c9607f18ff",
   "metadata": {},
   "source": [
    "[Julia](https://julialang.org/) is a scientific programming language that is free and open source for downloads, documentation, learning resources etc. Bridging high-level interpreted and low-level compiled languages, it offers high performance (comparable to C and Fortran) without sacrificing simplicity and programming productivity (like in Python or R).\n",
    "\n",
    "Julia has a rich ecosystem of libraries aimed towards scientific computing and a powerful built-in package manager to install and manage their dependencies. Julia is also gaining ground in HPC as it supports both multithreaded and distributed-memory parallelisation, as well as GPU computing.\n",
    "\n",
    "ENCCS offers learning materials for Julia programming:\n",
    "- [Introduction to programming in Julia](https://enccs.github.io/julia-intro/)\n",
    "- [Julia for High-Performance Data Analytics](https://enccs.github.io/julia-for-hpda/)\n",
    "- [Julia for High-Performance Scientific Computing](https://enccs.github.io/julia-for-hpc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e815b9d",
   "metadata": {},
   "source": [
    "## Short intro to GPU architecture\n",
    "\n",
    "GPUs are high-throughput accelerators separate from the main processor (CPU). They were originally developed to support the highly parallel computations needed for graphics processing, but are nowadays used to accelerate memory-local floating-point workloads. A GPU has a lot less control units and most of the die size is dedicated to floating point units, which execute tens of thousands of threads simultaneously. Data has to be manually moved from the *host* memory (i.e. RAM) to the *device* memory (i.e. video RAM, VRAM), which can be a performance bottleneck. Using GPUs makes sense when the performance gain from the parallel computation shadows the cost of data transfer to/from the accelerator. A comparison between CPU and GPU architectures is shown in the figure below:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://enccs.github.io/gpu-programming/_images/CPUAndGPU.png\" width=\"800\"/>\n",
    "<p>Ref: <a src=\"https://enccs.github.io/gpu-programming/\">ENCCS GPU programming lessons</a> </p>\n",
    "</div>\n",
    "\n",
    "Each thread has a local memory. Cores are grouped into streaming multiprocessors (CUDA)/compute units (ROCm), with a shared (CUDA)/private (ROCm) memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df4aa7-8f24-417d-9aae-e43e740112ed",
   "metadata": {},
   "source": [
    "## Julia and the two language problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e55a7d-138c-46bc-bd02-4753a74416b5",
   "metadata": {},
   "source": [
    "To run code in any programming language, some sort of translation into machine instructions needs to take place, but how this translation takes place differs between programming languages:\n",
    "- Interpreted languages like Python and R translate instructions line by line.\n",
    "- Compiled languages like C/C++ and Fortran are translated by a compiler prior to program execution.\n",
    "\n",
    "The benefits of interpreted languages are that they are easier to read and write because less information on aspects like types and array sizes needs to be provided. Programmer productivity is thus higher in interpreted languages, but compiled languages can perform faster by orders of magnitude because the compiler can perform optimizations during the translation to assembly. It is often the case that a high-level language is used for rapid prototyping and then the algorithm is rewritten in a compiled language for performance. This is also known as the *two-language problem*.\n",
    "\n",
    "In many ways Julia looks like an interpreted language, and mostly behaves like one. But before each function is executed, Julia’s LLVM compiler will compile it `“just in time” (JIT)`. More on that later. Thus, it gives the flexibility of an interpreted language and the execution speed of a compiled language at the cost of waiting a bit longer for the first execution of any function.\n",
    "\n",
    "Julia has been designed to be both fast and dynamic. In the words of its developers: \n",
    "\n",
    "> We want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled. (Did we mention it should be as fast as C?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb4c5a-6717-41f0-9ddc-3d277376e41c",
   "metadata": {},
   "source": [
    "## Julia Micro-Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018da37-1ee9-4b58-a2d1-10819e46164f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://julialang.org/assets/images/benchmarks.svg\", width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e55c65-a751-435a-8dfe-6db4ef02e6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Basic syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63433095-2950-4336-861d-b6d01c8ded61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d91f2-400c-48aa-85c8-2e1e7e0db95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 3.14\n",
    "println(A, \" --- \", typeof(A))\n",
    "\n",
    "B = 10\n",
    "println(B, \" --- \", typeof(B))\n",
    "\n",
    "C = true\n",
    "println(C, \" --- \", typeof(C))\n",
    "\n",
    "D = 3+4im\n",
    "println(D, \" --- \", typeof(D))\n",
    "\n",
    "E = \"Hello, Julia\"\n",
    "println(E, \" --- \", typeof(E))\n",
    "\n",
    "# supertypes and subtypes\n",
    "print(supertypes(Float64), \" --- \", subtypes(Int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b407026-ee91-4d08-8220-9d7746bd6646",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vectors and Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249f064-ade5-4baa-9d95-85396864b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [1, 2, 3, 4] # 4-element Vector{Int64}\n",
    "a2 = [i^3 for i in [1,2,3,4]] # Array comprehension\n",
    "\n",
    "m1 = [[1,2]  [4,5]  [7,8]] # 2×3 Matrix{Int64}\n",
    "m2 = zeros(4,4,4,4) # Zero 4×4×4×4 Array{Float64, 4}\n",
    "\n",
    "# broadcasting\n",
    "b1 = a1.^2\n",
    "b2 = a2 .- a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f419f8",
   "metadata": {},
   "source": [
    "## Structs\n",
    "We can also create a composite type (a struct), and create a new method for that type. The `Point` composite type below is furthermore *parametric*, where we restrict the type T to be a subtype of `Real`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Point{T<:Real}\n",
    "    x::T\n",
    "    y::T\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745477b4-6867-454a-9b0f-00f08db72676",
   "metadata": {},
   "source": [
    "## Loops and Conditionals\n",
    "\n",
    "`for` loops iterate over iterables, including types like `Range`, `Array`, `Set` and `Dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa81c99-5f70-473a-aff4-6a38c617c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5]\n",
    "    println(\"i = $i\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac6471-56af-435a-9844-50841c5f5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1 2; 3 4]\n",
    "# visit each index of A efficiently\n",
    "for i in eachindex(A)\n",
    "    println(\"i = $i, A[i] = $(A[i])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9576994-9b22-40fd-bb06-5b3865ebfa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k, v) in Dict(\"A\" => 1, \"B\" => 2, \"C\" => 3)\n",
    "    println(\"$k is $v\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2ff15-4aef-4a28-9428-78290b61e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 6\n",
    "if x > 5\n",
    "    println(\"x > 5\")\n",
    "elseif x < 5    # optional elseif\n",
    "    println(\"x < 5\")\n",
    "else            # optional else\n",
    "    println(\"x = 5\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb0c61-90ff-4d7d-84bf-86e8c78d996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "while n < 10\n",
    "    n += 1\n",
    "    println(n)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45321a3-314f-4a93-ab3e-40d2a64eeb4a",
   "metadata": {},
   "source": [
    "## Working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185f9db-c86d-44d5-8124-14ef11e84552",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"myfile.txt\", \"w\") do f\n",
    "    write(f, \"a line\")\n",
    "end\n",
    "\n",
    "open(\"myfile.txt\") do f\n",
    "    # read from file\n",
    "    lines = readlines(f)\n",
    "    println(lines)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2883a-6130-4ab5-9e4f-d8891f20b4b6",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de13fe-7909-4eba-8174-ba8905b21a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x,y)\n",
    "    x + y   # can also use \"return\" keyword\n",
    "end\n",
    "\n",
    "f(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b973d-e10b-4ca6-a99f-882cabacb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keyword arguments can be added after `;`\n",
    "\n",
    "function greet_dog(; greeting = \"Hi\", dog_name = \"Fido\")  # note the ;\n",
    "    println(\"$greeting $dog_name\")\n",
    "end\n",
    "\n",
    "greet_dog(dog_name = \"Coco\", greeting = \"Go fetch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592297f-ba61-4b8e-8251-a487d6742a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional arguments\n",
    "\n",
    "function date(y, m=2, d=2)\n",
    "    month = lpad(m, 2, \"0\")  # lpad pads from the left\n",
    "    day = lpad(d, 2, \"0\")\n",
    "    println(\"$y-$month-$day\")\n",
    "end\n",
    "\n",
    "date(2024)\n",
    "date(2024, 3)\n",
    "date(2024, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696df91-37f6-4579-982b-6916117ea338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument types can be specified explicitly\n",
    "# This is not really needed as things will be JITed automatically without input \n",
    "# needed\n",
    "function f(x::Float64, y::Float64)\n",
    "    return x*y\n",
    "end\n",
    "\n",
    "# return types can also be specified\n",
    "function g(x, y)::Int8\n",
    "    return x * y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484ee50-ed66-429f-a91c-3d3dab7014ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Special features of Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26faf6-5f4d-4dfd-8694-3bfd48b03910",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hierarchy Types\n",
    "\n",
    "Julia is a dynamically typed language and does not require users to explicitly declare types because types are inferred and used at runtime. The sophisticated type system helps Julia to generate efficient code.\n",
    "\n",
    "As types play a fundamental role in Julia’s design, it’s important to have a mental model of Julia’s type system. There are two basic kinds of types in Julia:\n",
    "- Abstract types which define the type hierarchy and interface that types have to expose;\n",
    "- Concrete types which describe data structures, that is, concrete implementations that can be used for variables.\n",
    "\n",
    "Furthermore, a **primitive type** consists of plain bits such as an integer, character or floating point number. A **parametric type** represents a set of types. Types in Julia form a “type tree”, in which the leaves are concrete types.\n",
    "For example, `Array` types can be concrete `DenseArray`, `SparseArray`, `CuArray`, but they all descend from `AbstractArray` as a supertype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8eac55-7762-4674-af72-0e9799cd96f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Julia-number-type-hierarchy.svg/1920px-Julia-number-type-hierarchy.svg.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e43ace-90e1-4b63-b005-477a81ea4da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acdc72-fd2b-441e-b474-303d3540c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sumsquare(x, y)\n",
    "    return x^2 + y^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db8e14-66d9-45df-994a-187c47fa04cb",
   "metadata": {},
   "source": [
    "Each function can have multiple methods. A method is a function defined for specific arguments types.\n",
    "Here we define methods using short form syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aebcb5-71df-4e60-811a-a111c881af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsquare(x::Float64, y::Float64) = x^2 + y^2\n",
    "sumsquare(x::Int64, y::Int64) = x^2 + y^2\n",
    "sumsquare(x::Complex{Float64}, y::Complex{Float64}) = x^2 + y^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949b98a-0ea1-423b-a722-e9a8d8777917",
   "metadata": {},
   "source": [
    "Julia’s type system also enables `multiple dispatch`, that is, choosing the most specific method of a function based on the argument types. \n",
    "\n",
    "`Multiple dispatch` sets the language apart from most other languages and makes it composable and fast when combined with just-in-time (JIT) compilation using the LLVM compiler toolchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13975166-0ddc-46ff-bfd7-20fd0cbd6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing arguments with all kinds of types to this function\n",
    "\n",
    "# Int64\n",
    "println(sumsquare(2, 3))\n",
    "\n",
    "# Float64\n",
    "println(sumsquare(2.72, 3.83))\n",
    "    \n",
    "# Complex{Float64}\n",
    "println(sumsquare(1.2+2.3im, 2.1-1.5im))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29f603-ba52-46a8-aa61-3404d8ddf000",
   "metadata": {},
   "source": [
    "This can also be extended to composite types, e.g. we can now create two variables of type `Point`, and define a method for `sumsquare` which accepts this type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e91b7e-6ea8-445b-a8dc-7801fc64a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sumsquare(p1::Point, p2::Point)\n",
    "   return Point(p1.x^2 + p2.x^2, p1.y^2 + p2.y^2)\n",
    "end\n",
    "\n",
    "p1, p2 = Point(1.0, 2.0), Point(2.0, 3.0)\n",
    "sumsquare(p1, p2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb40f18-6fa6-40e3-bb48-5f398dbbd9ac",
   "metadata": {},
   "source": [
    "## Metaprogramming\n",
    "\n",
    "Julia represents its own code as a data structure accessible from the language itself. Since code is represented by objects that can be created and manipulated from within the language, it is possible for a program to transform and generate its own code, that is to create powerful macros (the term \"metaprogramming\" refers to the possibility to write code that writes code that is then evaluated).\n",
    "\n",
    "Note the difference from C or C++ macros. There, macros work performing textual manipulation and substitution before any actual parsing or interpretation occurs.\n",
    "\n",
    "In Julia, macros work when the code has already been parsed and organised in a syntax tree, and hence the semantic is much richer and allows for much more powerful manipulations.\n",
    "For example, the `@btime` macro can pre prefixed to any code line/block to automatically generate and execute benchmarking code for a given snippet.\n",
    "\n",
    "More reading materials for the metaprogramming\n",
    "- [Documentation on metaprogramming](https://docs.julialang.org/en/v1/manual/metaprogramming/)\n",
    "- [Metaprogramming tutorial from JuliaCon21](https://github.com/dpsanders/Metaprogramming_JuliaCon_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26000d12-9ec2-4ad5-a34a-b91585862370",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. GPU programming using Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2811fe-dcdb-44ca-b9b1-c5589aeacbac",
   "metadata": {},
   "source": [
    "Julia has first-class support for GPU programming through the following packages that target GPUs from all major vendors:\n",
    "- [CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs\n",
    "- [AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs\n",
    "- [oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs\n",
    "- [Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs\n",
    "\n",
    "ENCCS reading materials:\n",
    "- [GPU Programming: When, Why and How?](https://enccs.github.io/gpu-programming/6-language-support/#julia)\n",
    "- [Julia for High-Performance Scientific Computing](https://enccs.github.io/julia-for-hpc/GPU/)\n",
    "- [Julia for High-Performance Data Analytics](https://enccs.github.io/julia-for-hpda/)\n",
    "\n",
    "Moreover, the [KernelAbstractions.jl](https://juliagpu.github.io/KernelAbstractions.jl/stable/) package can be useful to write vendor-agnostic code that can execute on different GPU brands (and also fallback on CPUs if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da2de8-54b5-4fbd-97d2-281af7fa5857",
   "metadata": {},
   "source": [
    "## Setup and Access to GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ab35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preliminary imports\n",
    "using Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.add(\"KernelAbstractions\")\n",
    "Pkg.add(\"ProgressMeter\")\n",
    "using Plots\n",
    "using BenchmarkTools\n",
    "using KernelAbstractions\n",
    "using ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0432bc-e4b5-4ccb-a9d2-1e4fbc3972b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I have an AMDGPU so I need the AMDGPU.jl package\n",
    "Pkg.add(\"AMDGPU\")\n",
    "# Is everything working?\n",
    "using AMDGPU\n",
    "AMDGPU.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46612e04-b392-4ac1-8c4f-ee4ddb2393c7",
   "metadata": {},
   "source": [
    "## The Array interface\n",
    "\n",
    "GPU programming with Julia can be as simple as using a different array type instead of regular Base.Array. \n",
    "\n",
    "For an AMD GPU, the `ROCArray` type from `AMDGPU.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75922a4c-bd6c-4e0e-a837-9f7a2692f686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions\n",
    "\n",
    "# copy an array to GPU and executes a simple operation on GPU\n",
    "A_d = ROCArray([1,2,3,4])\n",
    "A_d .+= 1\n",
    "\n",
    "# fetch data from GPU to CPU\n",
    "A = Array(A_d) # the overhead of copying data to GPU makes such simple calculations very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e94c9-d41f-4a37-9c65-e3776501fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code example for matrix multiplication\n",
    "using BenchmarkTools\n",
    "\n",
    "A = rand(2^9, 2^9);\n",
    "A_d = ROCArray(A);\n",
    "\n",
    "@btime $A * $A; # btime lets us benchmark the function\n",
    "\n",
    "@btime AMDGPU.@sync $A_d * $A_d; # GPU calls are async, so I need to put a barrier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673446f-941c-43da-892a-4f9466920958",
   "metadata": {},
   "source": [
    "## Writing your own kernels\n",
    "\n",
    "In this example we will sum two arrays A and B and store the result in C. The same code runs on both CPU and GPU depending on the type of arrays that is fed to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5cf8c-cc0a-4634-90de-1df0dac5204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function vadd!(C, @Const(A), @Const(B))\n",
    "    i = @index(Global)\n",
    "    @inbounds C[i] = A[i] + B[i]\n",
    "end\n",
    "\n",
    "function my_vadd!(C, A, B)\n",
    "    backend = get_backend(A)\n",
    "    kernel! = vadd!(backend)\n",
    "    kernel!(C,A,B, ndrange = size(C)) # Could be a one-liner vadd!(backend, 256)(C, A, B, ndrange=size(C))\n",
    "end\n",
    "\n",
    "A = zeros(10) .+ 3.0;\n",
    "B = ones(10) .* 2;\n",
    "C = similar(B);\n",
    "my_vadd!(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7ace8-fad4-4738-addd-55155e4aa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of vector addition kernel for AMD GPU\n",
    "\n",
    "A_d = ROCArray(A);\n",
    "B_d = ROCArray(B);\n",
    "C_d = similar(B_d);\n",
    "\n",
    "vadd!(C_d, A_d, B_d)\n",
    "C_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd3c9d",
   "metadata": {},
   "source": [
    "A few interesting points:\n",
    "- `@kernel` is a macro that identifies the function as a compilable kernel that can run on one of the supported backends (CUDA, ROCm, oneAPI, Metal, CPU);\n",
    "- The `@Const` macro is used to ensure to the runtime that the array will not be modified or aliased. This allows for optimised read access. It roughly corresponds to the `__restrict` CUDA C keyword;\n",
    "- The `@index(Global)` is used to obtain the index of the array element in the global workload (which in normal CUDA/HIP would need to be computed as `blockIdx.x * tile_dim + threadIdx.x`)\n",
    "- `ndrange` is the total size of the workgroup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaa091",
   "metadata": {},
   "source": [
    "## A more convoluted example\n",
    "\n",
    "Let us now try to solve a problem that can be encountered in computational physics: the heat equation, which takes the following form:\n",
    "\n",
    "$$ \\dfrac{\\partial T}{\\partial t} = \\alpha\\nabla^2T $$\n",
    "\n",
    "This equation is suitable to describe diffusion processes. We will try to solve this equation on a regular 2D grid using [stencils](https://en.wikipedia.org/wiki/Stencil_(numerical_analysis)). \n",
    "\n",
    "<div>\n",
    "<img src=\"https://enccs.github.io/gpu-programming/_images/stencil.svg\" width=\"800\">\n",
    "<p>Regular 2D grid where the heat equation is solved. The indices (i,j) indicate the spacial coordinates of the gridpoints, whereas the superscript m indicates the timestep.</p>\n",
    "</div>\n",
    "\n",
    "Let us define preliminary variables containing the problem size, timestep and initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = dy = 0.1\n",
    "N = 64\n",
    "nsteps = 500\n",
    "α = 10 # Diffusion coefficient\n",
    "dt = dx^2 * dy^2 / (2.0 * α * (dx^2 + dy^2)) # Largest stable time step\n",
    "T_disk = 200 # Temperature of the hot disk\n",
    "T_domain = 50\n",
    "\n",
    "# Now to generate the initial condition: a hot disk in the middle of the domain, colder everywhere else\n",
    "\n",
    "T = zeros(N+2,N+2) .+ T_domain # Initial condition everywhere\n",
    "# Now we can add our hot disk\n",
    "r²= (N/6)^2\n",
    "for i = 1:N+2\n",
    "    for j = 1:N+2\n",
    "        ds² = (i - N/2)^2 + (j - N/2)^2\n",
    "        if ds² < r²\n",
    "            T[i,j] = T_disk\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "T_prev = copy(T); # We will need the old timestep to do time marching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the real work horse: the diffusion kernel\n",
    "\n",
    "using KernelAbstractions\n",
    "\n",
    "@kernel function diffusion!(T, T_prev, dx, dy, α, dt, N)\n",
    "    i,j = @index(Global, NTuple)\n",
    "    if i > 1 && j > 1 && i < N+2 && j < N+2\n",
    "        @inbounds begin\n",
    "            ddx = (T_prev[i-1,j] - 2.0 * T_prev[i,j] + T_prev[i+1,j]) / (dx*dx)\n",
    "            ddy = (T_prev[i,j-1] - 2.0* T_prev[i,j] + T_prev[i, j+1]) / (dy*dy)\n",
    "            T[i,j] = T_prev[i,j] + α * dt * (ddx + ddy)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function evolve!(T, T_prev, dx, dy, α, dt, N)\n",
    "    backend = get_backend(T)\n",
    "    kernel! = diffusion!(backend)\n",
    "    kernel!(T, T_prev, dx, dy, α, dt, N, ndrange=(N,N))\n",
    "    KernelAbstractions.synchronize(backend)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run this on CPU\n",
    "#p = Progress(nsteps)\n",
    "display(heatmap(T, colorbar_title=\"Temperature\"))\n",
    "for i=1:nsteps\n",
    "    evolve!(T, T_prev, dx, dy, α, dt, N)\n",
    "    tmp = T\n",
    "    T = T_prev\n",
    "    T_prev = tmp # Swap T and T_prev\n",
    "    #next!(p)\n",
    "end\n",
    "display(heatmap(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same thing on GPU\n",
    "T = zeros(N+2,N+2) .+ T_domain # Initial condition everywhere\n",
    "# Now we can add our hot disk\n",
    "r²= (N/6)^2\n",
    "for i = 1:N+2\n",
    "    for j = 1:N+2\n",
    "        ds² = (i - N/2)^2 + (j - N/2)^2\n",
    "        if ds² < r²\n",
    "            T[i,j] = T_disk\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "T_prev = copy(T); # We will need the old timestep to do time marching\n",
    "\n",
    "T_d = ROCArray(T);\n",
    "T_prev_d = ROCArray(T_prev);\n",
    "display(heatmap(T, colorbar_title=\"Temperature\"))\n",
    "for i=1:nsteps\n",
    "    evolve!(T_d, T_prev_d, dx, dy, α, dt, N)\n",
    "    #heatmap!(T)\n",
    "    tmp_d = T_d;\n",
    "    T_d = T_prev_d;\n",
    "    T_prev_d = tmp_d; # Swap T and T_prev\n",
    "    #next!(p)\n",
    "end\n",
    "display(heatmap(Array(T_d)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdb760-0fcc-4d9c-87cd-748ae7178db3",
   "metadata": {},
   "source": [
    "**Restrictions in kernel programming**\n",
    "\n",
    "Within kernels, most of the Julia language is supported with the exception of functionality that requires the Julia runtime library. This means one cannot allocate memory or perform dynamic function calls, both of which are easy to do accidentally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b9040-5f29-44ef-b745-cb5f80c4276e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Julia packages for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe842a-2ed1-4446-99ef-bf1b29122369",
   "metadata": {},
   "source": [
    "The `MLJ.jl` (https://github.com/alan-turing-institute/MLJ.jl) package provides a unified interface to common machine learning algorithms, which include `generalized linear models`, `decision trees`, and `clustering`.\n",
    "\n",
    "`Flux.jl` (https://github.com/FluxML/Flux.jl) and `Knet.jl` (https://github.com/denizyuret/Knet.jl) are powerful packages for Deep Learning.\n",
    "\n",
    "Packages such as `Metalhead.jl`(https://github.com/FluxML/Metalhead.jl), `ObjectDetector.jl`(https://github.com/r3tex/ObjectDetector.jl), and `TextAnalysis.jl`(https://github.com/JuliaText/TextAnalysis.jl) provide ready to use pre-trained models for common tasks.\n",
    "\n",
    "`AlphaZero.jl`(https://github.com/jonathan-laurent/AlphaZero.jl) provides a high performance implementation of the reinforcement learning algorithms from AlphaZero.\n",
    "\n",
    "`Turing.jl`(https://turinglang.org/stable/) is a best in class package for probabilistic programming.\n",
    "\n",
    "More packages for AI & ML from Julia official website\n",
    "- ML: https://juliapackages.com/c/machine-learning\n",
    "- NLP: https://juliapackages.com/c/nlp\n",
    "- Neural Networks: https://juliapackages.com/c/neural-networks\n",
    "- Reinforcement Learning: https://juliapackages.com/c/reinforcement-learning\n",
    "- Supervised Learning: https://juliapackages.com/c/supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ea560-8d88-49cc-b360-fd800c3434fb",
   "metadata": {},
   "source": [
    "### Training a deep neural network to classify penguins using `Flux.jl`\n",
    "\n",
    "Flux.jl comes “batteries-included” with many useful tools built in, but also enables the user to write own Julia code for DL components.\n",
    "- Flux has relatively few explicit APIs for features like regularisation or embeddings.\n",
    "- All of Flux is straightforward Julia code and it can be worth to inspect and extend it if needed.\n",
    "- Flux works well with other Julia libraries, like dataframes, images and differential equation solvers. One can build complex data processing pipelines that integrate Flux models.\n",
    "\n",
    "To train a model we need four things:\n",
    "- A collection of data points that will be provided to the objective function.\n",
    "- A objective (cost or loss) function, that evaluates how well a model is doing given some input data.\n",
    "- The definition of a model and access to its trainable parameters.\n",
    "- An optimiser that will update the model parameters appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3b6cb-06de-4b07-85a7-e495aa67f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.add(\"Flux\")\n",
    "Pkg.add(\"MLJ\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"PalmerPenguins\")\n",
    "Pkg.add(\"StatsBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c71ea0-f659-46b4-b7ba-0a92057ddbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "using MLJ: partition, ConfusionMatrix\n",
    "using DataFrames\n",
    "using PalmerPenguins\n",
    "\n",
    "table = PalmerPenguins.load()\n",
    "df = DataFrame(table)\n",
    "dropmissing!(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592536f-d4a7-4849-b21b-c1aed2979b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of the Penguins dataset and making it suitable for training a network.\n",
    "\n",
    "# select feature and label columns\n",
    "X = select(df, Not([:species, :sex, :island]))\n",
    "Y = df[:, :species]\n",
    "\n",
    "# split into training and testing parts\n",
    "(xtrain, xtest), (ytrain, ytest) = partition((X, Y), 0.8, shuffle=true, rng=123, multi=true)\n",
    "\n",
    "# use single precision and transpose arrays\n",
    "xtrain, xtest = Float32.(Array(xtrain)'), Float32.(Array(xtest)')\n",
    "\n",
    "# one-hot encoding\n",
    "ytrain = Flux.onehotbatch(ytrain, [\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "ytest = Flux.onehotbatch(ytest, [\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "# count penguin classes to see if it's balanced\n",
    "sum(ytrain, dims=2)\n",
    "sum(ytest, dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac46d1-2923-4b51-ae7a-d405685b5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define `model` to be the neural network.\n",
    "\n",
    "n_features, n_classes, n_neurons = 4, 3, 10\n",
    "model = Chain(\n",
    "        Dense(n_features, n_neurons),\n",
    "        BatchNorm(n_neurons, relu),\n",
    "        Dense(n_neurons, n_classes),\n",
    "        softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c86ddc-c7e8-4ed1-b0c5-1a15d1e6900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a `loss` function which will be minimized during training.\n",
    "# Herein we use cross-entropy loss function typically used for classification\n",
    "loss(x, y) = Flux.crossentropy(model(x), y)\n",
    "\n",
    "# Define another function presenting the accuracy of the model\n",
    "# onecold (opposite to onehot) gives back the original representation\n",
    "function accuracy(x, y)\n",
    "    return sum(Flux.onecold(model(x)) .== Flux.onecold(y)) / size(y, 2)\n",
    "end\n",
    "\n",
    "# check accuracy before training\n",
    "accuracy(xtrain, ytrain)\n",
    "accuracy(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fbef2-31a5-401e-9ac6-41ae8c8d88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase: sample\n",
    "\n",
    "# Instead of training the entire dataset, training data were divided into `minibatches` and \n",
    "# update network weights on each minibatch separately.\n",
    "function create_minibatches(xtrain, ytrain, batch_size=32, n_batch=10)\n",
    "    minibatches = Tuple[]\n",
    "    for i in 1:n_batch\n",
    "        randinds = sample(1:size(xtrain, 2), batch_size)\n",
    "        push!(minibatches, (xtrain[:, randinds], ytrain[:,randinds]))\n",
    "    end\n",
    "    return minibatches\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc84d8-ab8a-4e7e-8974-8871316abcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an anonymous `callback` function to pass into the training function to monitor the progress, \n",
    "# to select standard ADAM optimizer, and to extract parameters of the model.\n",
    "\n",
    "callback = () -> @show(loss(xtrain, ytrain))\n",
    "opt = ADAM()\n",
    "θ = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f54b1-e23d-48eb-ba90-3b1370618b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is to create the `minibatches` to call the `create_minibatches` function defined above.\n",
    "minibatches = create_minibatches(xtrain, ytrain)\n",
    "\n",
    "# We run 100 epochs to train the model.\n",
    "for i in 1:100\n",
    "    # train on minibatches\n",
    "    Flux.train!(loss, θ, minibatches, opt, cb = Flux.throttle(callback, 1));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595db393-2f88-4a1d-bc73-2ec8b7f5a628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check final accuracy\n",
    "\n",
    "accuracy(xtrain, ytrain)\n",
    "accuracy(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0297612-c2eb-4900-b944-c0210932acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we create a confusion matrix to quantify the performance of the model.\n",
    "\n",
    "predicted_species = Flux.onecold(model(xtest), [\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "true_species = Flux.onecold(ytest, [\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "ConfusionMatrix()(predicted_species, true_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5f89a-005c-45ae-962e-aa422bb2ba9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
